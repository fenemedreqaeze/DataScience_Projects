{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Data collection and management : project\n",
    "## Planning my next holidays ‚òÄÔ∏è\n",
    "\n",
    "Let's create a script that allows to get some information about all the hotels in a given city on www.Booking.com üßô\n",
    "**We strongly recommend that you use Scrapy, it will be much easier !**\n",
    "\n",
    "You can scrap as many information asyou want, but we suggest that you get at least :\n",
    "* The hotel name, \n",
    "* The url to its booking.com page, \n",
    "* Its coordinates : latitude and longitude\n",
    "* The score given by the website users\n",
    "* The text description of the hotel\n",
    "\n",
    "Then, you can execute this script for several cities from yesterday's list. Make sure you save the results in different files for each city and that the name of the city is stored in the filename (because you will use it later üòâ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_name = 'La Rochelle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hotels(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"hotels\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'ss': destination_name},\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        hotels = response.css('.sr_item')\n",
    "\n",
    "        for h in hotels:\n",
    "            yield {\n",
    "                'name': h.css('.sr-hotel__name::text').get(),\n",
    "                'url': \"https://www.booking.com\" + h.css('.hotel_name_link').attrib[\"href\"],\n",
    "                'coords': h.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                'score': h.css('.bui-review-score__badge::text').get(),\n",
    "                'description': h.css('.hotel_desc::text').get()\n",
    "                \n",
    "            }\n",
    "        \n",
    "        \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-04 11:29:05 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-04 11:29:05 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-04 11:29:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-04 11:29:05 [scrapy.extensions.telnet] INFO: Telnet Password: a2a9b1d235118e82\n",
      "2020-09-04 11:29:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-04 11:29:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-04 11:29:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-04 11:29:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-04 11:29:05 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-04 11:29:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-04 11:29:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-04 11:29:18 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-09-04 11:29:18 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-04 11:29:18 [scrapy.extensions.feedexport] INFO: Stored json feed (260 items) in: res/2_hotels_La-Rochelle.json\n",
      "2020-09-04 11:29:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 19045,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 1762089,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 12,\n",
      " 'elapsed_time_seconds': 12.853263,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 4, 11, 29, 18, 120621),\n",
      " 'item_scraped_count': 260,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75972608,\n",
      " 'memusage/startup': 75972608,\n",
      " 'request_depth_max': 11,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2020, 9, 4, 11, 29, 5, 267358)}\n",
      "2020-09-04 11:29:18 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"2_hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir('res/'):\n",
    "        os.remove('res/' + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'res/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
